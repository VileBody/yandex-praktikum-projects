{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотел бы не тратить время на лемматизацию при проработке вариантов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data = pd.read_csv('/datasets/toxic_comments.csv')\\ndata\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\"\"\"data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\"\"\"for i in range(data.shape[0]):\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "    data.loc[i, 'lemmatized'] = ''.join(m.lemmatize((''.join(re.sub(r'[^a-zA-Z]', ' ', data.loc[i, 'text'].lower())))))[:-1]\n",
    "data\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data.to_csv('toxic_data.csv')\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww  he matches this background colour i m s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man  i m really not trying to edit war  it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestions on im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you  sir  are my hero  any chance you remember...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159566</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of asking  when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159567</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself   that is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159568</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer   umm  theres no actual article for pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159569</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159570</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>and     i really don t think you understand ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  \":::::And for the second time of asking, when ...      0   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "                                               lemmatized  \n",
       "0       explanation why the edits made under my userna...  \n",
       "1       d aww  he matches this background colour i m s...  \n",
       "2       hey man  i m really not trying to edit war  it...  \n",
       "3         more i can t make any real suggestions on im...  \n",
       "4       you  sir  are my hero  any chance you remember...  \n",
       "...                                                   ...  \n",
       "159566        and for the second time of asking  when ...  \n",
       "159567  you should be ashamed of yourself   that is a ...  \n",
       "159568  spitzer   umm  theres no actual article for pr...  \n",
       "159569  and it looks like it was actually you who put ...  \n",
       "159570    and     i really don t think you understand ...  \n",
       "\n",
       "[159571 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('toxic_data.csv', index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предлагаю разделить датасет на трейн и тест заранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>28291</td>\n",
       "      <td>hey guess what!!!! BS again buddy. i know how ...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey guess what     bs again buddy  i know how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91597</td>\n",
       "      <td>Austin Bourke was a meteorologist. He was invo...</td>\n",
       "      <td>0</td>\n",
       "      <td>austin bourke was a meteorologist  he was invo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67544</td>\n",
       "      <td>Don't hide the incriminating photographs you N...</td>\n",
       "      <td>1</td>\n",
       "      <td>don t hide the incriminating photographs you n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131756</td>\n",
       "      <td>Friendly Discussion ==\\n\\nDear Kumarrao,\\n\\nI ...</td>\n",
       "      <td>0</td>\n",
       "      <td>friendly discussion     dear kumarrao   i had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136401</td>\n",
       "      <td>So, for this limited goal (dimension is time o...</td>\n",
       "      <td>0</td>\n",
       "      <td>so  for this limited goal  dimension is time o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108822</td>\n",
       "      <td>I'm all for changing it to BCE and CE from BC ...</td>\n",
       "      <td>0</td>\n",
       "      <td>i m all for changing it to bce and ce from bc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83278</td>\n",
       "      <td>\"\\n\\nI was blocked for harrassment and disrupt...</td>\n",
       "      <td>0</td>\n",
       "      <td>i was blocked for harrassment and disruptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136848</td>\n",
       "      <td>\"\\n\\n Example \\n\\nOkay, heres what you should ...</td>\n",
       "      <td>0</td>\n",
       "      <td>example   okay  heres what you should do  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74109</td>\n",
       "      <td>Invitation \\n\\nG'day Thewanderer, just wanted ...</td>\n",
       "      <td>0</td>\n",
       "      <td>invitation   g day thewanderer  just wanted to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64400</td>\n",
       "      <td>\"\\n\\nI have answered all the points, tendentio...</td>\n",
       "      <td>0</td>\n",
       "      <td>i have answered all the points  tendentious...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127656 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "28291   hey guess what!!!! BS again buddy. i know how ...      0   \n",
       "91597   Austin Bourke was a meteorologist. He was invo...      0   \n",
       "67544   Don't hide the incriminating photographs you N...      1   \n",
       "131756  Friendly Discussion ==\\n\\nDear Kumarrao,\\n\\nI ...      0   \n",
       "136401  So, for this limited goal (dimension is time o...      0   \n",
       "...                                                   ...    ...   \n",
       "108822  I'm all for changing it to BCE and CE from BC ...      0   \n",
       "83278   \"\\n\\nI was blocked for harrassment and disrupt...      0   \n",
       "136848  \"\\n\\n Example \\n\\nOkay, heres what you should ...      0   \n",
       "74109   Invitation \\n\\nG'day Thewanderer, just wanted ...      0   \n",
       "64400   \"\\n\\nI have answered all the points, tendentio...      0   \n",
       "\n",
       "                                               lemmatized  \n",
       "28291   hey guess what     bs again buddy  i know how ...  \n",
       "91597   austin bourke was a meteorologist  he was invo...  \n",
       "67544   don t hide the incriminating photographs you n...  \n",
       "131756  friendly discussion     dear kumarrao   i had ...  \n",
       "136401  so  for this limited goal  dimension is time o...  \n",
       "...                                                   ...  \n",
       "108822  i m all for changing it to bce and ce from bc ...  \n",
       "83278      i was blocked for harrassment and disruptio...  \n",
       "136848      example   okay  heres what you should do  ...  \n",
       "74109   invitation   g day thewanderer  just wanted to...  \n",
       "64400      i have answered all the points  tendentious...  \n",
       "\n",
       "[127656 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, train_size=0.8)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>67544</td>\n",
       "      <td>Don't hide the incriminating photographs you N...</td>\n",
       "      <td>1</td>\n",
       "      <td>don t hide the incriminating photographs you n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127285</td>\n",
       "      <td>I dont like you\\nYou are like a Wiki Nazi alwa...</td>\n",
       "      <td>1</td>\n",
       "      <td>i dont like you you are like a wiki nazi alway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24263</td>\n",
       "      <td>Wikipedia is the worst idea ever.  It will nev...</td>\n",
       "      <td>1</td>\n",
       "      <td>wikipedia is the worst idea ever   it will nev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145464</td>\n",
       "      <td>Your DO know that Hispanics are part of Europe...</td>\n",
       "      <td>1</td>\n",
       "      <td>your do know that hispanics are part of europe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109421</td>\n",
       "      <td>Your country is shit!!! All it does is pollute...</td>\n",
       "      <td>1</td>\n",
       "      <td>your country is shit    all it does is pollute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65988</td>\n",
       "      <td>Why Wikpedia Is Gay\\n\\nWikipedia sucks. I will...</td>\n",
       "      <td>1</td>\n",
       "      <td>why wikpedia is gay  wikipedia sucks  i will n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53272</td>\n",
       "      <td>YO\\nWHY CANT I MESS UP UR SITE\\nNO SCHOOLS R A...</td>\n",
       "      <td>1</td>\n",
       "      <td>yo why cant i mess up ur site no schools r all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121419</td>\n",
       "      <td>666 = 2x above Holy 333 Trinity  \\n\\nrepent ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>x above holy     trinity    repent bastard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79281</td>\n",
       "      <td>\"\\n\\n pakis - one of the most ugliest individu...</td>\n",
       "      <td>1</td>\n",
       "      <td>pakis   one of the most ugliest individual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38205</td>\n",
       "      <td>You stupid and more liar than me you don't wan...</td>\n",
       "      <td>1</td>\n",
       "      <td>you stupid and more liar than me you don t wan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12985 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "67544   Don't hide the incriminating photographs you N...      1   \n",
       "127285  I dont like you\\nYou are like a Wiki Nazi alwa...      1   \n",
       "24263   Wikipedia is the worst idea ever.  It will nev...      1   \n",
       "145464  Your DO know that Hispanics are part of Europe...      1   \n",
       "109421  Your country is shit!!! All it does is pollute...      1   \n",
       "...                                                   ...    ...   \n",
       "65988   Why Wikpedia Is Gay\\n\\nWikipedia sucks. I will...      1   \n",
       "53272   YO\\nWHY CANT I MESS UP UR SITE\\nNO SCHOOLS R A...      1   \n",
       "121419  666 = 2x above Holy 333 Trinity  \\n\\nrepent ba...      1   \n",
       "79281   \"\\n\\n pakis - one of the most ugliest individu...      1   \n",
       "38205   You stupid and more liar than me you don't wan...      1   \n",
       "\n",
       "                                               lemmatized  \n",
       "67544   don t hide the incriminating photographs you n...  \n",
       "127285  i dont like you you are like a wiki nazi alway...  \n",
       "24263   wikipedia is the worst idea ever   it will nev...  \n",
       "145464  your do know that hispanics are part of europe...  \n",
       "109421  your country is shit    all it does is pollute...  \n",
       "...                                                   ...  \n",
       "65988   why wikpedia is gay  wikipedia sucks  i will n...  \n",
       "53272   yo why cant i mess up ur site no schools r all...  \n",
       "121419         x above holy     trinity    repent bastard  \n",
       "79281       pakis   one of the most ugliest individual...  \n",
       "38205   you stupid and more liar than me you don t wan...  \n",
       "\n",
       "[12985 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_toxic = train[train['toxic']==1]\n",
    "train_toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406735 5281081\n"
     ]
    }
   ],
   "source": [
    "words_used_in_toxic = []\n",
    "words_used_in_nontoxic = []\n",
    "\n",
    "for i in train.index:\n",
    "    buf = train.loc[i]\n",
    "    phrase = set(buf['lemmatized'].split())\n",
    "    if buf['toxic']==1:\n",
    "        words_used_in_toxic+=list(phrase)\n",
    "    else:\n",
    "        words_used_in_nontoxic+=list(phrase)\n",
    "toxic_dict = pd.Series(words_used_in_toxic).value_counts()\n",
    "nontoxic_dict = pd.Series(words_used_in_nontoxic).value_counts()\n",
    "print(len(words_used_in_toxic), len(words_used_in_nontoxic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_used_in_toxic = set(words_used_in_toxic)\n",
    "words_used_in_nontoxic = set(words_used_in_nontoxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_used_ONLY_in_toxic = words_used_in_toxic - words_used_in_nontoxic\n",
    "words_used_ONLY_in_nontoxic = words_used_in_nontoxic - words_used_in_toxic\n",
    "words_used_everywhere = words_used_in_toxic & words_used_in_nontoxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "words_used_everywhere-=set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_dict = dict(toxic_dict)\n",
    "nontoxic_dict = dict(nontoxic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country       0.084746\n",
       "treatment     0.035714\n",
       "apologises    0.333333\n",
       "spartacus     0.166667\n",
       "musicians     0.020408\n",
       "                ...   \n",
       "activists     0.046154\n",
       "userpages     0.057692\n",
       "wba           0.500000\n",
       "provided      0.033230\n",
       "cult          0.120000\n",
       "Length: 22452, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantity = pd.Series()\n",
    "for word in list(words_used_everywhere):\n",
    "    quantity[word] = toxic_dict[word]/nontoxic_dict[word]\n",
    "quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemm_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>28291</td>\n",
       "      <td>hey guess what!!!! BS again buddy. i know how ...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey guess what     bs again buddy  i know how ...</td>\n",
       "      <td>noone#a#say#spend#can#hey#they#to#its#mason#kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91597</td>\n",
       "      <td>Austin Bourke was a meteorologist. He was invo...</td>\n",
       "      <td>0</td>\n",
       "      <td>austin bourke was a meteorologist  he was invo...</td>\n",
       "      <td>weird#visitation#agricultural#a#great#thesis#a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67544</td>\n",
       "      <td>Don't hide the incriminating photographs you N...</td>\n",
       "      <td>1</td>\n",
       "      <td>don t hide the incriminating photographs you n...</td>\n",
       "      <td>trying#incriminating#think#to#graphic#its#phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131756</td>\n",
       "      <td>Friendly Discussion ==\\n\\nDear Kumarrao,\\n\\nI ...</td>\n",
       "      <td>0</td>\n",
       "      <td>friendly discussion     dear kumarrao   i had ...</td>\n",
       "      <td>historian#always#of#distinguish#can#p#urs#much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136401</td>\n",
       "      <td>So, for this limited goal (dimension is time o...</td>\n",
       "      <td>0</td>\n",
       "      <td>so  for this limited goal  dimension is time o...</td>\n",
       "      <td>format#may#reduced#sortkey#recognised#could#ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108822</td>\n",
       "      <td>I'm all for changing it to BCE and CE from BC ...</td>\n",
       "      <td>0</td>\n",
       "      <td>i m all for changing it to bce and ce from bc ...</td>\n",
       "      <td>bc#i#changing#m#all#it#from#bce#for#and#to#ce#ad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83278</td>\n",
       "      <td>\"\\n\\nI was blocked for harrassment and disrupt...</td>\n",
       "      <td>0</td>\n",
       "      <td>i was blocked for harrassment and disruptio...</td>\n",
       "      <td>removes#indicating#wished#block#agreed#quite#o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136848</td>\n",
       "      <td>\"\\n\\n Example \\n\\nOkay, heres what you should ...</td>\n",
       "      <td>0</td>\n",
       "      <td>example   okay  heres what you should do  ...</td>\n",
       "      <td>put#ask#example#do#monkey#ref#can#could#okay#e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74109</td>\n",
       "      <td>Invitation \\n\\nG'day Thewanderer, just wanted ...</td>\n",
       "      <td>0</td>\n",
       "      <td>invitation   g day thewanderer  just wanted to...</td>\n",
       "      <td>send#us#merrier#to#join#invite#at#g#over#day#j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64400</td>\n",
       "      <td>\"\\n\\nI have answered all the points, tendentio...</td>\n",
       "      <td>0</td>\n",
       "      <td>i have answered all the points  tendentious...</td>\n",
       "      <td>munafiq#do#all#of#now#though#most#mean#have#th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127656 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "28291   hey guess what!!!! BS again buddy. i know how ...      0   \n",
       "91597   Austin Bourke was a meteorologist. He was invo...      0   \n",
       "67544   Don't hide the incriminating photographs you N...      1   \n",
       "131756  Friendly Discussion ==\\n\\nDear Kumarrao,\\n\\nI ...      0   \n",
       "136401  So, for this limited goal (dimension is time o...      0   \n",
       "...                                                   ...    ...   \n",
       "108822  I'm all for changing it to BCE and CE from BC ...      0   \n",
       "83278   \"\\n\\nI was blocked for harrassment and disrupt...      0   \n",
       "136848  \"\\n\\n Example \\n\\nOkay, heres what you should ...      0   \n",
       "74109   Invitation \\n\\nG'day Thewanderer, just wanted ...      0   \n",
       "64400   \"\\n\\nI have answered all the points, tendentio...      0   \n",
       "\n",
       "                                               lemmatized  \\\n",
       "28291   hey guess what     bs again buddy  i know how ...   \n",
       "91597   austin bourke was a meteorologist  he was invo...   \n",
       "67544   don t hide the incriminating photographs you n...   \n",
       "131756  friendly discussion     dear kumarrao   i had ...   \n",
       "136401  so  for this limited goal  dimension is time o...   \n",
       "...                                                   ...   \n",
       "108822  i m all for changing it to bce and ce from bc ...   \n",
       "83278      i was blocked for harrassment and disruptio...   \n",
       "136848      example   okay  heres what you should do  ...   \n",
       "74109   invitation   g day thewanderer  just wanted to...   \n",
       "64400      i have answered all the points  tendentious...   \n",
       "\n",
       "                                                 lemm_set  \n",
       "28291   noone#a#say#spend#can#hey#they#to#its#mason#kn...  \n",
       "91597   weird#visitation#agricultural#a#great#thesis#a...  \n",
       "67544   trying#incriminating#think#to#graphic#its#phot...  \n",
       "131756  historian#always#of#distinguish#can#p#urs#much...  \n",
       "136401  format#may#reduced#sortkey#recognised#could#ca...  \n",
       "...                                                   ...  \n",
       "108822   bc#i#changing#m#all#it#from#bce#for#and#to#ce#ad  \n",
       "83278   removes#indicating#wished#block#agreed#quite#o...  \n",
       "136848  put#ask#example#do#monkey#ref#can#could#okay#e...  \n",
       "74109   send#us#merrier#to#join#invite#at#g#over#day#j...  \n",
       "64400   munafiq#do#all#of#now#though#most#mean#have#th...  \n",
       "\n",
       "[127656 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_copy = train.copy()\n",
    "train_copy.loc[:, 'lemm_set'] = 0\n",
    "j=0\n",
    "for i in train_copy.index:\n",
    "    if j%1000==0:\n",
    "        print(j)\n",
    "    j+=1\n",
    "    train_copy.loc[i, 'lemm_set'] = '#'.join(list(set(train_copy.loc[i, 'lemmatized'].split())))\n",
    "train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* 8 *************************\n",
      "7125\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "8987 0.9707970682546955 8724.553252404949\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for itr in [8]: \n",
    "    print('*'*25, itr, '*'*25)\n",
    "    train_copy.loc[:, 'mark'] = 0\n",
    "    toxic_words = set(list(quantity[quantity>itr].index) + list(words_used_ONLY_in_toxic))\n",
    "    print(len(toxic_words))\n",
    "    j=0\n",
    "    for i in train_copy.index:\n",
    "        if j%10000==0:\n",
    "            print(j)\n",
    "        j+=1\n",
    "        for word in set(train_copy.loc[i, 'lemm_set'].split('#')):\n",
    "            if word in toxic_words:\n",
    "                train_copy.loc[i, 'mark'] = 1\n",
    "                continue\n",
    "    a = train_copy[train_copy['mark']==1].shape[0]\n",
    "    b = f1_score(train_copy[train_copy['mark']==1]['toxic'], train_copy[train_copy['mark']==1]['mark'])\n",
    "    print(a, b , a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle = quantity[(quantity<=8) & (quantity>0.01)].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "************************* 0 *************************\n",
      "13684\n",
      "1.3455605073705863\n",
      "0.778697695233833\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "************************* 1 *************************\n",
      "14850\n",
      "1.2141046667660653\n",
      "0.7781602373887241\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "************************* 2 *************************\n",
      "15974\n",
      "1.112404125892621\n",
      "0.7774153074027603\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "************************* 3 *************************\n",
      "21408\n",
      "0.8046025457304223\n",
      "0.7981082844096543\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "************************* 4 *************************\n",
      "27002\n",
      "0.6173704702006588\n",
      "0.8167453314180175\n"
     ]
    }
   ],
   "source": [
    "train_copy_middle = train_copy.copy()\n",
    "train_copy_middle.loc[:, 'mark_2'] = 0\n",
    "param = 25\n",
    "for itr in range(5):\n",
    "    filling = set(middle[(middle.shape[0]//param)*itr:(middle.shape[0]//param)*(itr+1)].index)\n",
    "    j=0\n",
    "    for i in train_copy_middle[train_copy_middle['mark_2']==0].index:\n",
    "        if j%10000==0:\n",
    "            print(j)\n",
    "        j+=1\n",
    "        for word in set(train_copy_middle.loc[i, 'lemm_set'].split('#')):\n",
    "            if word in filling:\n",
    "                train_copy_middle.loc[i, 'mark_2'] = 1\n",
    "    \n",
    "    print('*'*25, itr, '*'*25)\n",
    "    print(train_copy_middle[train_copy_middle['mark_2']==1].shape[0])\n",
    "    print(train_copy_middle[(train_copy_middle['mark_2']==1) & (train_copy_middle['toxic']==1)].shape[0]/train_copy_middle[(train_copy_middle['mark_2']==1) & (train_copy_middle['toxic']==0)].shape[0])\n",
    "    print(f1_score(\n",
    "        train_copy_middle[train_copy_middle['mark_2']==0]['toxic'],\n",
    "        train_copy_middle[train_copy_middle['mark_2']==0]['mark']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntb_checked = set(middle[:(middle.shape[0]//param)*(itr+1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemm_set</th>\n",
       "      <th>mark</th>\n",
       "      <th>mark_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>91597</td>\n",
       "      <td>Austin Bourke was a meteorologist. He was invo...</td>\n",
       "      <td>0</td>\n",
       "      <td>austin bourke was a meteorologist  he was invo...</td>\n",
       "      <td>weird#visitation#agricultural#a#great#thesis#a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67544</td>\n",
       "      <td>Don't hide the incriminating photographs you N...</td>\n",
       "      <td>1</td>\n",
       "      <td>don t hide the incriminating photographs you n...</td>\n",
       "      <td>trying#incriminating#think#to#graphic#its#phot...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131756</td>\n",
       "      <td>Friendly Discussion ==\\n\\nDear Kumarrao,\\n\\nI ...</td>\n",
       "      <td>0</td>\n",
       "      <td>friendly discussion     dear kumarrao   i had ...</td>\n",
       "      <td>historian#always#of#distinguish#can#p#urs#much...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127285</td>\n",
       "      <td>I dont like you\\nYou are like a Wiki Nazi alwa...</td>\n",
       "      <td>1</td>\n",
       "      <td>i dont like you you are like a wiki nazi alway...</td>\n",
       "      <td>put#always#do#a#some#good#all#delete#people#co...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64700</td>\n",
       "      <td>\"\\n\\n Friday FAC \\n\\nThanks for the comments, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>friday fac   thanks for the comments  that...</td>\n",
       "      <td>always#weasel#of#disdain#could#values#substitu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133588</td>\n",
       "      <td>Dangers of Aluminum Bats \\n\\nWithout citations...</td>\n",
       "      <td>0</td>\n",
       "      <td>dangers of aluminum bats   without citations t...</td>\n",
       "      <td>aren#own#of#bat#caused#appear#happened#have#th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38205</td>\n",
       "      <td>You stupid and more liar than me you don't wan...</td>\n",
       "      <td>1</td>\n",
       "      <td>you stupid and more liar than me you don t wan...</td>\n",
       "      <td>cease#of#official#renounce#to#took#lie#website...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45122</td>\n",
       "      <td>Nah, the leftist bias in this article makes th...</td>\n",
       "      <td>0</td>\n",
       "      <td>nah  the leftist bias in this article makes th...</td>\n",
       "      <td>article#wing#a#any#leftist#thing#sad#makes#s#w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83278</td>\n",
       "      <td>\"\\n\\nI was blocked for harrassment and disrupt...</td>\n",
       "      <td>0</td>\n",
       "      <td>i was blocked for harrassment and disruptio...</td>\n",
       "      <td>removes#indicating#wished#block#agreed#quite#o...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136848</td>\n",
       "      <td>\"\\n\\n Example \\n\\nOkay, heres what you should ...</td>\n",
       "      <td>0</td>\n",
       "      <td>example   okay  heres what you should do  ...</td>\n",
       "      <td>put#ask#example#do#monkey#ref#can#could#okay#e...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27002 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "91597   Austin Bourke was a meteorologist. He was invo...      0   \n",
       "67544   Don't hide the incriminating photographs you N...      1   \n",
       "131756  Friendly Discussion ==\\n\\nDear Kumarrao,\\n\\nI ...      0   \n",
       "127285  I dont like you\\nYou are like a Wiki Nazi alwa...      1   \n",
       "64700   \"\\n\\n Friday FAC \\n\\nThanks for the comments, ...      0   \n",
       "...                                                   ...    ...   \n",
       "133588  Dangers of Aluminum Bats \\n\\nWithout citations...      0   \n",
       "38205   You stupid and more liar than me you don't wan...      1   \n",
       "45122   Nah, the leftist bias in this article makes th...      0   \n",
       "83278   \"\\n\\nI was blocked for harrassment and disrupt...      0   \n",
       "136848  \"\\n\\n Example \\n\\nOkay, heres what you should ...      0   \n",
       "\n",
       "                                               lemmatized  \\\n",
       "91597   austin bourke was a meteorologist  he was invo...   \n",
       "67544   don t hide the incriminating photographs you n...   \n",
       "131756  friendly discussion     dear kumarrao   i had ...   \n",
       "127285  i dont like you you are like a wiki nazi alway...   \n",
       "64700       friday fac   thanks for the comments  that...   \n",
       "...                                                   ...   \n",
       "133588  dangers of aluminum bats   without citations t...   \n",
       "38205   you stupid and more liar than me you don t wan...   \n",
       "45122   nah  the leftist bias in this article makes th...   \n",
       "83278      i was blocked for harrassment and disruptio...   \n",
       "136848      example   okay  heres what you should do  ...   \n",
       "\n",
       "                                                 lemm_set  mark  mark_2  \n",
       "91597   weird#visitation#agricultural#a#great#thesis#a...     0       1  \n",
       "67544   trying#incriminating#think#to#graphic#its#phot...     1       1  \n",
       "131756  historian#always#of#distinguish#can#p#urs#much...     0       1  \n",
       "127285  put#always#do#a#some#good#all#delete#people#co...     1       1  \n",
       "64700   always#weasel#of#disdain#could#values#substitu...     0       1  \n",
       "...                                                   ...   ...     ...  \n",
       "133588  aren#own#of#bat#caused#appear#happened#have#th...     0       1  \n",
       "38205   cease#of#official#renounce#to#took#lie#website...     0       1  \n",
       "45122   article#wing#a#any#leftist#thing#sad#makes#s#w...     0       1  \n",
       "83278   removes#indicating#wished#block#agreed#quite#o...     0       1  \n",
       "136848  put#ask#example#do#monkey#ref#can#could#okay#e...     0       1  \n",
       "\n",
       "[27002 rows x 6 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_model = train_copy_middle[train_copy_middle['mark_2']==1]\n",
    "data_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = data_for_model['lemmatized'].values.astype('U')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "features_train = count_tf_idf.fit_transform(corpus)\n",
    "target_train = data_for_model['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LogisticRegression()\n",
    "model2.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "model = lgb.LGBMClassifier()\n",
    "model.fit(features_train, target_train)\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>mark</th>\n",
       "      <th>mark_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>68588</td>\n",
       "      <td>who cares, they are worthless scum</td>\n",
       "      <td>1</td>\n",
       "      <td>who cares  they are worthless scum</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49379</td>\n",
       "      <td>@JHJ, the only sense in which CBS Records is S...</td>\n",
       "      <td>0</td>\n",
       "      <td>jhj  the only sense in which cbs records is s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39969</td>\n",
       "      <td>And just to be clear, an interview with an ano...</td>\n",
       "      <td>0</td>\n",
       "      <td>and just to be clear  an interview with an ano...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35926</td>\n",
       "      <td>RE: your head page...!\\nLOL!</td>\n",
       "      <td>0</td>\n",
       "      <td>re  your head page     lol</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47779</td>\n",
       "      <td>Nephilim deleted edit \\n\\nYou state reliable s...</td>\n",
       "      <td>0</td>\n",
       "      <td>nephilim deleted edit   you state reliable sou...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146621</td>\n",
       "      <td>I wrote a letter complaining about you Acroter...</td>\n",
       "      <td>0</td>\n",
       "      <td>i wrote a letter complaining about you acroter...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145397</td>\n",
       "      <td>Could a section be added to the milan bit \\n\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>could a section be added to the milan bit   ju...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126732</td>\n",
       "      <td>\"\\n\\nI like to know where I stand when someone...</td>\n",
       "      <td>0</td>\n",
       "      <td>i like to know where i stand when someone m...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29638</td>\n",
       "      <td>Spaceman spiff,may god be remove your mental p...</td>\n",
       "      <td>0</td>\n",
       "      <td>spaceman spiff may god be remove your mental p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156251</td>\n",
       "      <td>\" (UTC)\\n\\nI have a friend who got 3 years of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>utc   i have a friend who got   years of ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31915 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "68588                  who cares, they are worthless scum      1   \n",
       "49379   @JHJ, the only sense in which CBS Records is S...      0   \n",
       "39969   And just to be clear, an interview with an ano...      0   \n",
       "35926                        RE: your head page...!\\nLOL!      0   \n",
       "47779   Nephilim deleted edit \\n\\nYou state reliable s...      0   \n",
       "...                                                   ...    ...   \n",
       "146621  I wrote a letter complaining about you Acroter...      0   \n",
       "145397  Could a section be added to the milan bit \\n\\n...      0   \n",
       "126732  \"\\n\\nI like to know where I stand when someone...      0   \n",
       "29638   Spaceman spiff,may god be remove your mental p...      0   \n",
       "156251  \" (UTC)\\n\\nI have a friend who got 3 years of ...      0   \n",
       "\n",
       "                                               lemmatized  mark  mark_2  \n",
       "68588                  who cares  they are worthless scum     1       1  \n",
       "49379    jhj  the only sense in which cbs records is s...     0       0  \n",
       "39969   and just to be clear  an interview with an ano...     0       0  \n",
       "35926                         re  your head page     lol      0       0  \n",
       "47779   nephilim deleted edit   you state reliable sou...     0       1  \n",
       "...                                                   ...   ...     ...  \n",
       "146621  i wrote a letter complaining about you acroter...     0       0  \n",
       "145397  could a section be added to the milan bit   ju...     0       0  \n",
       "126732     i like to know where i stand when someone m...     1       1  \n",
       "29638   spaceman spiff may god be remove your mental p...     0       0  \n",
       "156251     utc   i have a friend who got   years of ch...     0       1  \n",
       "\n",
       "[31915 rows x 5 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_copy = test.copy()\n",
    "test_copy.loc[:, 'mark'] = 0\n",
    "test_copy.loc[:, 'mark_2'] = 0\n",
    "for i in test_copy.index:\n",
    "    for word in set(test_copy.loc[i, 'lemmatized'].split()):\n",
    "        if word in toxic_words:\n",
    "            test_copy.loc[i, 'mark'] = 1\n",
    "            continue\n",
    "        if word in ntb_checked:\n",
    "        \n",
    "            test_copy.loc[i, 'mark_2'] = 1\n",
    "            features_test = count_tf_idf.transform(pd.Series(test_copy.loc[i, 'lemmatized']).astype('U'))\n",
    "            test_copy.loc[i, 'mark'] = round((model.predict(features_test)[0] + model2.predict(features_test)[0]+0.1)/2)\n",
    "            continue\n",
    "test_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2474 27831 844 766\n"
     ]
    }
   ],
   "source": [
    "true_positive, true_negative, false_positive, false_negative = 0, 0, 0, 0\n",
    "t = test_copy.copy().reset_index(drop=True)\n",
    "for i in range(t.shape[0]):\n",
    "    if (t.loc[i, 'toxic']==1) and (t.loc[i, 'mark']==1):\n",
    "        true_positive+=1\n",
    "    if (t.loc[i, 'toxic']==1) and (t.loc[i, 'mark']==0):\n",
    "        false_negative+=1\n",
    "    if (t.loc[i, 'toxic']==0) and (t.loc[i, 'mark']==1):\n",
    "        false_positive+=1\n",
    "    if (t.loc[i, 'toxic']==0) and (t.loc[i, 'mark']==0):\n",
    "        true_negative+=1\n",
    "print(true_positive, true_negative, false_positive, false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8090493980904938\n",
      "0.7544983226593475 -- итог\n"
     ]
    }
   ],
   "source": [
    "#print(f1_score(test_copy[test_copy['ntb']==1]['toxic'], test_copy[test_copy['ntb']==1]['mark']))\n",
    "print(f1_score(test_copy[test_copy['mark_2']==1]['toxic'], test_copy[test_copy['mark_2']==1]['mark']))\n",
    "print(f1_score(test_copy['toxic'], test_copy['mark']), '-- итог')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из спортивного интереса решил выполнить проект на сервере, а не на локальной машине или не в гугл колабе. Но датасет такой большой, что не то что при попытке запустить берта, при попытке перегнать датасет через tf-idf кернел умирал. Почувствовав себя советским программистом, решил написать, как я думаю, велосипед. Идея заключается в следующем: разделить датасет на 3 части, одну из которых я буду прогонять через модель (использовал tf-idf, lightGBMClassifier, LogisitcsRegression). В двух оставшихся prediction будет состоять из единичек или из нулей.\n",
    "\n",
    "Как я распределял датасет на части: \n",
    "Вообще сначала хотел использовать берта, потому что вот эта строка \"использовать BERT необязательно\" изрядно меня раззадорила. Но потом я понял, что берт для данного проекта слишком продвинутый инструмент (а еще мне было лениво придумывать как распаковать файл tar.gz): нам не нужно определять смысл каждой фразы, ведь если в высказывании условно содержится одно из всемирно известных четырехбуквенных слов, то она будет токсичной, независимо от посыла. Да, жестко, но мне кажется, справедливо, ведь это не соцсеть, а интернет магазин. Я составил словарь слов которые использовались ТОЛЬКО в токсичных высказываниях (разумеется трейнового датасета), добавил к ним слова которые чаще (более чем в 1 раз) использовались в токсичных высказываниях, чем в нетоксичных. И тут я получил нож в спину от разметчиков. Почему то этот интернет магазин совсем не против обсуждения политики в своих комментариях. Я обнаружил несколько (уверен их было куда больше) политических высказываний, на острые темы, которые можно считать токсичными или нетоксичными в зависимости от позиции визави комментатора. Еще раз повторюсь, это не соцсеть. Почему вот тут такое происходит?\n",
    "\n",
    "В реальной жизни я бы отправился уточнять задачу, но в данной ситуации датасет не поменять\n",
    "Поэтому я избрал стратегию которую описал выше. Разделил на три части датасет. \"Верхняя\" часть -- токсичная, должна была удовлетворять одному условию: максимально возможное значение d1_score (97 меня устроило). \"Нтжняя\" -- нетоксичная: должна быть максимально большой и опять таки, f1 должна быть значительно выше 75 процентов. Зачем f1 больше 75%? Сейчас  работаю с трейном, я могу добиться точности и близкой к ста на нем. Но когда я перейду на тест, результат может заметно ухудшиться из-за того, что при составлении словаря \"токсичных\" слов я не пользовался бутстрапом, и словарь для теста может довольно сильно отличаться от трейнового словаря.\n",
    "\n",
    "\"Средняя\" часть трейна должна быть небольшой, чтобы кернел не умер, достаточно большой, чтобы с запасом закрыть вот эту \"неоднозначную\" часть теста и должна содержать примерно поровну 1 и 0 на трейне (а значит что то похожее и на тесте), так регрессия сработает лучше. Постарался максимально соблюсти все эти условия. \n",
    "\n",
    "Когда все это выполнил, результат был около 74,3, усреднением двух моделей добился пересечения порога в 75.\n",
    "Одним из преимуществ моего подхода является возможность внедрения самообучения, поскольку в моей программе есть словарь, в который можно добавлять отсутствующие в нем слова и исправлять соотношение использования в токсичных/нетоксичных случаях. Голь на выдумку хитра конечно, столько оскорблений я до выполнения этого проекта не видел, но в принципе, если внедрить самообучение и длительное время обучать программу, пополняя словарь, можно добиться  почти идеальной ее работы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
